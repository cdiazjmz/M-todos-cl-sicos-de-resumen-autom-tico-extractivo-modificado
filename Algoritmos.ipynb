{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6d3467",
   "metadata": {},
   "source": [
    "LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy.linalg import svd as singular_value_decomposition\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import docx\n",
    "import re\n",
    "\n",
    "# Descargar recursos necesarios si no están ya descargados\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class LsaSummarizer:\n",
    "    MIN_DIMENSIONS = 3\n",
    "    REDUCTION_RATIO = 1 / 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self._stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = words\n",
    "\n",
    "    def __call__(self, document_text, sentences_count, important_words):\n",
    "        dictionary = self._create_dictionary(document_text)\n",
    "        if not dictionary:\n",
    "            return []\n",
    "\n",
    "        sentences = sent_tokenize(document_text)\n",
    "\n",
    "        matrix = self._create_matrix(document_text, dictionary, important_words)\n",
    "        matrix = self._compute_term_frequency(matrix)\n",
    "        u, sigma, v = singular_value_decomposition(matrix, full_matrices=False)\n",
    "\n",
    "        ranks = iter(self._compute_ranks(sigma, v))\n",
    "        return self._get_best_sentences(sentences, sentences_count, important_words, lambda s: next(ranks))\n",
    "\n",
    "    def _create_dictionary(self, document):\n",
    "        words = word_tokenize(document)\n",
    "        words = map(self.normalize_word, words)\n",
    "        unique_words = frozenset(w for w in words if w not in self._stop_words and w)\n",
    "        return dict((w, i) for i, w in enumerate(unique_words))\n",
    "\n",
    "    def _create_matrix(self, document, dictionary, important_words):\n",
    "        sentences = sent_tokenize(document)\n",
    "        words_count = len(dictionary)\n",
    "        sentences_count = len(sentences)\n",
    "        matrix = np.zeros((words_count, sentences_count))\n",
    "        for col, sentence in enumerate(sentences):\n",
    "            words = word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                normalized = self.normalize_word(word)\n",
    "                if normalized in dictionary:\n",
    "                    row = dictionary[normalized]\n",
    "                    matrix[row, col] += 1\n",
    "                    if normalized in important_words:\n",
    "                        matrix[row, col] *= 2\n",
    "        return matrix\n",
    "\n",
    "    def _compute_term_frequency(self, matrix, smooth=0.4):\n",
    "        max_word_frequencies = np.max(matrix, axis=0)\n",
    "        rows, cols = matrix.shape\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                max_word_frequency = max_word_frequencies[col]\n",
    "                if max_word_frequency != 0:\n",
    "                    frequency = matrix[row, col] / max_word_frequency\n",
    "                    matrix[row, col] = smooth + (1.0 - smooth) * frequency\n",
    "        return matrix\n",
    "\n",
    "    def _compute_ranks(self, sigma, v_matrix):\n",
    "        dimensions = max(self.MIN_DIMENSIONS, int(len(sigma) * self.REDUCTION_RATIO))\n",
    "        powered_sigma = [s**2 if i < dimensions else 0.0 for i, s in enumerate(sigma)]\n",
    "\n",
    "        ranks = []\n",
    "        for column_vector in v_matrix.T:\n",
    "            rank = sum(s * v**2 for s, v in zip(powered_sigma, column_vector))\n",
    "            ranks.append(math.sqrt(rank))\n",
    "        return ranks\n",
    " \n",
    "\n",
    "    def _get_best_sentences(self, sentences, sentences_count, important_words, rank_fn):\n",
    "        ranked_sentences = [(rank_fn(sentence), sentence) for sentence in sentences]\n",
    "        ranked_sentences.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        # Identificar las oraciones que contienen las palabras relevantes\n",
    "        relevant_sentences = [sentence for sentence in sentences if any(word in sentence for word in important_words)]\n",
    "\n",
    "        # Tomar las oraciones relevantes primero\n",
    "        selected_sentences = relevant_sentences[:sentences_count]\n",
    "\n",
    "        # Si necesitamos más oraciones, añadir las de mayor puntaje\n",
    "        additional_sentences_needed = sentences_count - len(selected_sentences)\n",
    "        if additional_sentences_needed > 0:\n",
    "            remaining_sentences = [sentence for _, sentence in ranked_sentences if sentence not in selected_sentences]\n",
    "            selected_sentences.extend(remaining_sentences[:additional_sentences_needed])\n",
    "\n",
    "        return selected_sentences\n",
    "\n",
    "    def normalize_word(self, word):\n",
    "        return ''.join(e for e in word if e.isalnum()).lower()\n",
    "    \n",
    "    def normalize_word(self, word):\n",
    "        return ''.join(e for e in word if e.isalnum()).lower()\n",
    "\n",
    "# Función para leer .docx y retornar texto\n",
    "def read_docx_text(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# Función principal\n",
    "if __name__ == '__main__':\n",
    "    ruta_docx = \"Case Study 3.docx\"\n",
    "    text = read_docx_text(ruta_docx)\n",
    "\n",
    "    keywords = [\"need\", \"want\", \"have\", \"has\", \"should\", \"require\", \"demand\", \"must\"]\n",
    "    lower_keywords = [k.lower() for k in keywords]\n",
    "\n",
    "    # Contar oraciones con palabras clave\n",
    "    all_sentences = sent_tokenize(text)\n",
    "    count = 0\n",
    "    for s in all_sentences:\n",
    "        words = [w.strip(string.punctuation).lower() for w in word_tokenize(s)]\n",
    "        if any(k in words for k in lower_keywords):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Se encontraron {count} oraciones que contienen alguna de las palabras clave: {keywords}\")\n",
    "\n",
    "    # Generar resumen\n",
    "    summarizer = LsaSummarizer()\n",
    "    resumen = summarizer(text, count, important_words=lower_keywords)\n",
    "\n",
    "summary_textLsaMod = ' '.join(resumen)\n",
    "print(summary_textLsaMod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0d5e8",
   "metadata": {},
   "source": [
    "LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8856b5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "from collections import Counter, namedtuple\n",
    "from operator import attrgetter\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "def null_stemmer(word):\n",
    "    return word\n",
    "\n",
    "def to_unicode(text, encoding='utf-8', errors='strict'):\n",
    "    if isinstance(text, bytes):\n",
    "        return text.decode(encoding, errors=errors)\n",
    "    return text\n",
    "\n",
    "class ItemsCount(object):\n",
    "    def __init__(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def __call__(self, seq):\n",
    "        return seq[:self.count]\n",
    "\n",
    "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
    "\n",
    "class AbstractSummarizer(object):\n",
    "    def __init__(self, stemmer=null_stemmer):\n",
    "        if not callable(stemmer):\n",
    "            raise ValueError(\"Stemmer has to be a callable object\")\n",
    "        self._stemmer = stemmer\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
    "\n",
    "    def stem_word(self, word):\n",
    "        return self._stemmer(self.normalize_word(word))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_word(word):\n",
    "        return to_unicode(word).lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            def rate(s): return rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "                 for o, s in enumerate(sentences))\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        if not callable(count):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "        return tuple(i.sentence for i in infos)\n",
    "\n",
    "class LexRankSummarizer(AbstractSummarizer):\n",
    "    threshold = 0.1\n",
    "    epsilon = 0.01\n",
    "    _stop_words = frozenset()\n",
    "    _important_words = {\"need\", \"want\", \"have\", \"has\", \"should\", \"require\", \"demand\", \"must\"}\n",
    "   \n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        self._ensure_dependencies_installed()\n",
    "\n",
    "        sentences_words = [self._to_words_set(s) for s in document.sentences]\n",
    "        if not sentences_words:\n",
    "            return tuple()\n",
    "\n",
    "        tf_metrics = self._compute_tf(sentences_words)\n",
    "        idf_metrics = self._compute_idf(sentences_words)\n",
    "\n",
    "        matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)\n",
    "        scores = self.power_method(matrix, self.epsilon)\n",
    "        ratings = dict(zip(document.sentences, scores))\n",
    "\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dependencies_installed():\n",
    "        if numpy is None:\n",
    "            raise ValueError(\"LexRank summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
    "\n",
    "    def _to_words_set(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    def _compute_tf(self, sentences):\n",
    "        tf_values = map(Counter, sentences)\n",
    "        tf_metrics = []\n",
    "\n",
    "        for sentence in tf_values:\n",
    "            metrics = {}\n",
    "            max_tf = self._find_tf_max(sentence)\n",
    "\n",
    "            for term, tf in sentence.items():\n",
    "                weight = 2.0 if term in self._important_words else 1.0  # Peso adicional\n",
    "                metrics[term] = (tf * weight) / max_tf\n",
    "\n",
    "            tf_metrics.append(metrics)\n",
    "\n",
    "        return tf_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_tf_max(terms):\n",
    "        return max(terms.values()) if terms else 1\n",
    "\n",
    "    def _compute_idf(self, sentences):\n",
    "        idf_metrics = {}\n",
    "        sentences_count = len(sentences)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for term in sentence:\n",
    "                if term not in idf_metrics:\n",
    "                    n_j = sum(1 for s in sentences if term in s)\n",
    "                    boost = 1.5 if term in self._important_words else 1.0\n",
    "                    idf_metrics[term] = boost * math.log(sentences_count / (1 + n_j))\n",
    "\n",
    "        return idf_metrics\n",
    "\n",
    "    def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):\n",
    "        sentences_count = len(sentences)\n",
    "        matrix = numpy.zeros((sentences_count, sentences_count))\n",
    "        degrees = numpy.zeros((sentences_count,))\n",
    "\n",
    "        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):\n",
    "            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):\n",
    "                similarity = self.cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics)\n",
    "                if similarity > threshold:\n",
    "                    matrix[row, col] = 1.0\n",
    "                    degrees[row] += 1\n",
    "                else:\n",
    "                    matrix[row, col] = 0\n",
    "\n",
    "        for row in range(sentences_count):\n",
    "            for col in range(sentences_count):\n",
    "                if degrees[row] == 0:\n",
    "                    degrees[row] = 1\n",
    "                matrix[row][col] = matrix[row][col] / degrees[row]\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "        unique_words1 = frozenset(sentence1)\n",
    "        unique_words2 = frozenset(sentence2)\n",
    "        common_words = unique_words1 & unique_words2\n",
    "\n",
    "        numerator = 0.0\n",
    "        for term in common_words:\n",
    "            numerator += tf1[term] * tf2[term] * idf_metrics[term] ** 2\n",
    "\n",
    "        denominator1 = sum((tf1[t] * idf_metrics[t]) ** 2 for t in unique_words1)\n",
    "        denominator2 = sum((tf2[t] * idf_metrics[t]) ** 2 for t in unique_words2)\n",
    "\n",
    "        if denominator1 > 0 and denominator2 > 0:\n",
    "            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    @staticmethod\n",
    "    def power_method(matrix, epsilon):\n",
    "        transposed_matrix = matrix.T\n",
    "        sentences_count = len(matrix)\n",
    "        p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "        lambda_val = 1.0\n",
    "\n",
    "        while lambda_val > epsilon:\n",
    "            next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "            next_p /= numpy.linalg.norm(next_p)\n",
    "            lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "\n",
    "        return p_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499ba0b",
   "metadata": {},
   "source": [
    "Edmunson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5e6fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EdmundsonSummarizer(AbstractSummarizer):\n",
    "    def __init__(self, stemmer=None):\n",
    "        super().__init__(stemmer=stemmer)\n",
    "        self._bonus_words = []\n",
    "        self._stigma_words = []\n",
    "        self._null_words = []\n",
    "        self._keywords = []\n",
    "        self._cue_method = EdmundsonCueMethod()\n",
    "        self._key_method = EdmundsonKeyMethod()\n",
    "        self._title_method = EdmundsonTitleMethod()\n",
    "        self._location_method = EdmundsonLocationMethod()\n",
    "\n",
    " \n",
    "\n",
    "    def __call__(self, document, sentences_count=5):\n",
    "        self._cue_method.set_bonus_words(self._bonus_words)\n",
    "        self._cue_method.set_stigma_words(self._stigma_words)\n",
    "        self._cue_method.set_null_words(self._null_words)\n",
    "        self._key_method.set_keywords(self._keywords)\n",
    "        self._title_method.set_headings(document.headings)\n",
    "\n",
    "        rated_sentences = []\n",
    "        for sentence in document.sentences:\n",
    "            rating = (\n",
    "                1.0 * self._cue_method(sentence)\n",
    "                + 4.0 * self._key_method(sentence)  # <-- más peso a keywords\n",
    "                + 1.0 * self._title_method(sentence)\n",
    "                + 1.0 * self._location_method(sentence)\n",
    "            )\n",
    "            rated_sentences.append((rating, sentence))\n",
    "        rated_sentences.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [s for _, s in rated_sentences[:sentences_count]]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def bonus_words(self):\n",
    "        return self._bonus_words\n",
    "\n",
    "    @bonus_words.setter\n",
    "    def bonus_words(self, words):\n",
    "        self._bonus_words = ffilter(lambda w: w not in self._stop_words, map(self.stemmer, words))\n",
    "\n",
    "    @property\n",
    "    def stigma_words(self):\n",
    "        return self._stigma_words\n",
    "\n",
    "    @stigma_words.setter\n",
    "    def stigma_words(self, words):\n",
    "        self._stigma_words = ffilter(lambda w: w not in self._stop_words, map(self.stemmer, words))\n",
    "\n",
    "    @property\n",
    "    def null_words(self):\n",
    "        return self._null_words\n",
    "\n",
    "    @null_words.setter\n",
    "    def null_words(self, words):\n",
    "        self._null_words = ffilter(lambda w: w not in self._stop_words, map(self.stemmer, words))\n",
    "\n",
    "    @property\n",
    "    def keywords(self):\n",
    "        return self._keywords\n",
    "\n",
    "    @keywords.setter\n",
    "    def keywords(self, words):\n",
    "        self._keywords = ffilter(lambda w: w not in self._stop_words, map(self.stemmer, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa5740",
   "metadata": {},
   "source": [
    "TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f7673",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "from collections import Counter, namedtuple\n",
    "from operator import attrgetter\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "def null_stemmer(word):\n",
    "    return word\n",
    "\n",
    "def to_unicode(text, encoding='utf-8', errors='strict'):\n",
    "    if isinstance(text, bytes):\n",
    "        return text.decode(encoding, errors=errors)\n",
    "    return text\n",
    "\n",
    "class ItemsCount(object):\n",
    "    def __init__(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def __call__(self, seq):\n",
    "        return seq[:self.count]\n",
    "\n",
    "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
    "\n",
    "\n",
    "class AbstractSummarizer(object):\n",
    "    def __init__(self, stemmer=null_stemmer):\n",
    "        if not callable(stemmer):\n",
    "            raise ValueError(\"Stemmer has to be a callable object\")\n",
    "\n",
    "        self._stemmer = stemmer\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
    "\n",
    "    def stem_word(self, word):\n",
    "        return self._stemmer(self.normalize_word(word))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_word(word):\n",
    "        return to_unicode(word).lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            def rate(s): return rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "            for o, s in enumerate(sentences))\n",
    "\n",
    "        # sort sentences by rating in descending order\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        # get `count` first best rated sentences\n",
    "        if not callable(count):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        # sort sentences by their order in document\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "\n",
    "        return tuple(i.sentence for i in infos)\n",
    "\n",
    "\n",
    "\n",
    "class TextRankSummarizer(AbstractSummarizer):\n",
    "    \"\"\"An implementation of TextRank algorithm for summarization.\n",
    "\n",
    "    Source: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "    \"\"\"\n",
    "    epsilon = 1e-4\n",
    "    damping = 0.85\n",
    "    # small number to prevent zero-division error, see https://github.com/miso-belica/sumy/issues/112\n",
    "    _ZERO_DIVISION_PREVENTION = 1e-7\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        self._ensure_dependencies_installed()\n",
    "        if not document.sentences:\n",
    "            return ()\n",
    "\n",
    "        ratings = self.rate_sentences(document)\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dependencies_installed():\n",
    "        if numpy is None:\n",
    "            raise ValueError(\"TextRank summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
    "\n",
    "    def rate_sentences(self, document):\n",
    "        matrix = self._create_matrix(document)\n",
    "        ranks = self.power_method(matrix, self.epsilon)\n",
    "        print(ranks)\n",
    "        return {sent: rank for sent, rank in zip(document.sentences, ranks)}\n",
    "\n",
    "    def _create_matrix(self, document):\n",
    "    \n",
    "        sentences_as_words = [self._to_words_set(sent) for sent in document.sentences]\n",
    "        sentences_count = len(sentences_as_words)\n",
    "        weights = numpy.zeros((sentences_count, sentences_count))\n",
    "\n",
    "        for i, words_i in enumerate(sentences_as_words):\n",
    "            for j in range(i, sentences_count):\n",
    "                rating = self._rate_sentences_edge(words_i, sentences_as_words[j])\n",
    "                weights[i, j] = rating\n",
    "                weights[j, i] = rating\n",
    "\n",
    "        weights /= (weights.sum(axis=1)[:, numpy.newaxis] + self._ZERO_DIVISION_PREVENTION)\n",
    "\n",
    "        return numpy.full((sentences_count, sentences_count), (1.-self.damping) / sentences_count) \\\n",
    "               + self.damping * weights\n",
    "\n",
    "    def _to_words_set(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    @staticmethod\n",
    "    def _rate_sentences_edge(words1, words2):\n",
    "        KEYWORDS = {\"need\", \"want\", \"have\", \"has\", \"should\", \"require\", \"demand\", \"must\"}\n",
    "        rank = sum(words2.count(w) for w in words1)\n",
    "        if rank == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Bonus por presencia de palabras clave\n",
    "        keyword_bonus = 0\n",
    "        if any(w in KEYWORDS for w in words1 + words2):\n",
    "            keyword_bonus = 2  # puedes ajustar este valor\n",
    "\n",
    "        norm = math.log(len(words1)) + math.log(len(words2))\n",
    "        if numpy.isclose(norm, 0.):\n",
    "            return float(rank + keyword_bonus)\n",
    "        else:\n",
    "            return (rank + keyword_bonus) / norm\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def power_method(matrix, epsilon):\n",
    "        transposed_matrix = matrix.T\n",
    "        sentences_count = len(matrix)\n",
    "        p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "        lambda_val = 1.0\n",
    "\n",
    "        while lambda_val > epsilon:\n",
    "            next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "\n",
    "        return p_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0760e",
   "metadata": {},
   "source": [
    "Luhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d27866",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from collections import namedtuple\n",
    "from operator import attrgetter\n",
    "\n",
    "# Asegúrate de tener instalada la librería 'nltk' si planeas usar stemming\n",
    "# Puedes instalarla con: pip install nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Clase auxiliar para el modelo de frecuencia de términos\n",
    " # Palabras con refuerzo especial\n",
    "SPECIAL_TERMS = {\"need\", \"want\", \"have\", \"has\", \"should\", \"require\", \"demand\", \"must\"}\n",
    "SPECIAL_TERM_WEIGHT = 3  # Puedes ajustar este valor según el impacto que quieras\n",
    "\n",
    "class TfDocumentModel:\n",
    "    def __init__(self, words):\n",
    "        self._term_counts = {}\n",
    "        for word in words:\n",
    "            normalized = word.lower()\n",
    "            base_weight = 1\n",
    "            if normalized in SPECIAL_TERMS:\n",
    "                base_weight = SPECIAL_TERM_WEIGHT\n",
    "            self._term_counts[normalized] = self._term_counts.get(normalized, 0) + base_weight\n",
    "\n",
    "    def term_frequency(self, term):\n",
    "        return self._term_counts.get(term, 0)\n",
    "\n",
    "    def most_frequent_terms(self, count):\n",
    "        sorted_terms = sorted(self._term_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        return [term for term, freq in sorted_terms[:count]]\n",
    "\n",
    "# Función auxiliar para el stemming (si no se proporciona un stemmer)\n",
    "def null_stemmer(word):\n",
    "    return word\n",
    "\n",
    "# Función auxiliar para convertir a unicode (si es necesario para Python 2)\n",
    "def to_unicode(text):\n",
    "    if isinstance(text, bytes):\n",
    "        return text.decode('utf-8')\n",
    "    return str(text)\n",
    "\n",
    "# Clase auxiliar para contar items (si 'count' no es un entero)\n",
    "class ItemsCount:\n",
    "    def __init__(self, count):\n",
    "        self.count = count\n",
    "\n",
    "    def __call__(self, items):\n",
    "        return items[:self.count]\n",
    "\n",
    "SentenceInfo = namedtuple(\"SentenceInfo\", (\"sentence\", \"order\", \"rating\",))\n",
    "\n",
    "class AbstractSummarizer(object):\n",
    "    def __init__(self, stemmer=null_stemmer):\n",
    "        if not callable(stemmer):\n",
    "            raise ValueError(\"Stemmer has to be a callable object\")\n",
    "        self._stemmer = stemmer\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        raise NotImplementedError(\"This method should be overriden in subclass\")\n",
    "\n",
    "    def stem_word(self, word):\n",
    "        return self._stemmer(self.normalize_word(word))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_word(word):\n",
    "        return to_unicode(word).lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_best_sentences(sentences, count, rating, *args, **kwargs):\n",
    "        rate = rating\n",
    "        if isinstance(rating, dict):\n",
    "            assert not args and not kwargs\n",
    "            def rate(s): return rating[s]\n",
    "\n",
    "        infos = (SentenceInfo(s, o, rate(s, *args, **kwargs))\n",
    "                 for o, s in enumerate(sentences))\n",
    "\n",
    "        # sort sentences by rating in descending order\n",
    "        infos = sorted(infos, key=attrgetter(\"rating\"), reverse=True)\n",
    "        # get `count` first best rated sentences\n",
    "        if not callable(count):\n",
    "            count = ItemsCount(count)\n",
    "        infos = count(infos)\n",
    "        # sort sentences by their order in document\n",
    "        infos = sorted(infos, key=attrgetter(\"order\"))\n",
    "\n",
    "        return tuple(i.sentence for i in infos)\n",
    "\n",
    "class LuhnSummarizer(AbstractSummarizer):\n",
    "    max_gap_size = 4\n",
    "    # TODO: better recognition of significant words (automatic)\n",
    "    significant_percentage = 1\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        words = self._get_significant_words(document.words)\n",
    "        return self._get_best_sentences(document.sentences,\n",
    "                                        sentences_count, self.rate_sentence, words)\n",
    "\n",
    "    def _get_significant_words(self, words):\n",
    "        words = map(self.normalize_word, words)\n",
    "        words = tuple(self.stem_word(w) for w in words if w not in self._stop_words)\n",
    "\n",
    "        model = TfDocumentModel(words)\n",
    "\n",
    "        # take only best `significant_percentage` % words\n",
    "        best_words_count = int(len(words) * self.significant_percentage)\n",
    "        words = model.most_frequent_terms(best_words_count)\n",
    "\n",
    "        # take only words contained multiple times in document\n",
    "        return tuple(t for t in words if model.term_frequency(t) > 1)\n",
    "\n",
    "    def rate_sentence(self, sentence, significant_stems):\n",
    "        ratings = self._get_chunk_ratings(sentence, significant_stems)\n",
    "        return max(ratings) if ratings else 0\n",
    "\n",
    "    def _get_chunk_ratings(self, sentence, significant_stems):\n",
    "        chunks = []\n",
    "        NONSIGNIFICANT_CHUNK = [0]*self.max_gap_size\n",
    "\n",
    "        in_chunk = False\n",
    "        for order, word in enumerate(sentence.words):\n",
    "            stem = self.stem_word(word)\n",
    "            # new chunk\n",
    "            if stem in significant_stems and not in_chunk:\n",
    "                in_chunk = True\n",
    "                chunks.append([1])\n",
    "            # append word to chunk\n",
    "            elif in_chunk:\n",
    "                is_significant_word = int(stem in significant_stems)\n",
    "                chunks[-1].append(is_significant_word)\n",
    "\n",
    "            # end of chunk\n",
    "            if chunks and chunks[-1][-self.max_gap_size:] == NONSIGNIFICANT_CHUNK:\n",
    "                in_chunk = False\n",
    "\n",
    "        return tuple(map(self._get_chunk_rating, chunks))\n",
    "\n",
    "    def _get_chunk_rating(self, chunk):\n",
    "        chunk = self.__remove_trailing_zeros(chunk)\n",
    "        words_count = len(chunk)\n",
    "        assert words_count > 0\n",
    "\n",
    "        significant_words = sum(chunk)\n",
    "        if significant_words == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return significant_words**2 / words_count\n",
    "\n",
    "    def __remove_trailing_zeros(self, collection):\n",
    "        \"\"\"Removes trailing zeroes from indexable collection of numbers\"\"\"\n",
    "        index = len(collection) - 1\n",
    "        while index >= 0 and collection[index] == 0:\n",
    "            index -= 1\n",
    "        return collection[:index + 1]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
